<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>역전파 알고리즘의 이해</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/monokai.css">
    <style>
        :root {
            --main-color: #3498db;
            --accent-color: #2c3e50;
            --highlight-color: #e74c3c;
            --light-bg: #f8f9fa;
        }
        .reveal {
            font-family: 'Noto Sans KR', sans-serif;
            color: #333;
            font-size: 1.3em;
        }
        .reveal h1, .reveal h2, .reveal h3 {
            color: var(--accent-color);
            font-family: 'Noto Sans KR', sans-serif;
        }
        .reveal h1 {
            font-size: 2.99em;
            margin-bottom: 0.5em;
            color: var(--main-color);
        }
        .reveal h2 {
            font-size: 2.21em;
            color: var(--main-color);
            margin-bottom: 0.5em;
        }
        .reveal h3 {
            font-size: 1.69em;
            color: var(--accent-color);
            margin-bottom: 0.4em;
        }
        .reveal .slides {
            text-align: left;
        }
        .reveal .slides section {
            padding: 23px;
            height: 100%;
        }
        .definition {
            background-color: #e8f4f8;
            padding: 16px;
            border-radius: 6px;
            border-left: 5px solid var(--main-color);
            margin: 23px 0;
        }
        .formula {
            background-color: var(--light-bg);
            padding: 10px;
            border-radius: 5px;
            text-align: center;
            margin: 16px 0;
        }
        .step-box {
            border: 1px solid #ddd;
            padding: 16px;
            margin: 16px 0;
            border-radius: 10px;
            background-color: var(--light-bg);
        }
        .highlight {
            background-color: #fffacd;
            padding: 3px 5px;
            border-radius: 4px;
        }
        .key-point {
            font-weight: bold;
            color: var(--highlight-color);
        }
        .note {
            background-color: #ffe8e8;
            padding: 12px;
            border-radius: 8px;
            margin: 16px 0;
            font-size: 0.9em;
        }
        .chain-rule {
            display: flex;
            flex-direction: row;
            flex-wrap: wrap;
            justify-content: center;
            align-items: center;
            margin: 23px 0;
            text-align: center;
        }
        .chain-rule-step {
            flex: 1;
            min-width: 72px;
            border: 1px solid #ddd;
            border-radius: 10px;
            margin: 5px;
            padding: 10px;
            background-color: #fff;
        }
        .arrow {
            margin: 0 10px;
            font-size: 29px;
            color: var(--main-color);
        }
        .two-column {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
        }
        .column {
            flex: 1;
            padding: 0 10px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.92em;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 9px 13px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .footer {
            position: absolute;
            bottom: 10px;
            left: 10px;
            font-size: 0.55em;
            color: #888;
        }
        ul, ol {
            margin: 0.7em 0;
            padding-left: 2.2em;
        }
        li {
            margin-bottom: 0.5em;
        }
        .reveal ul ul, .reveal ul ol, .reveal ol ol, .reveal ol ul {
            margin-top: 0.25em;
            margin-bottom: 0.25em;
        }
        .reveal .slides > section,
        .reveal .slides > section > section {
            min-height: 100%;
            width: 100%;
            box-sizing: border-box;
        }
        .compact-text {
            font-size: 0.92em;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- 슬라이드 1: 제목 슬라이드 -->
            <section data-background="linear-gradient(to right, #3498db, #2c3e50)">
                <h1 style="color: white;">역전파 알고리즘의 이해</h1>
                <h3 style="color: #f0f0f0;">신경망 학습의 핵심 원리</h3>
            </section>

            <!-- 슬라이드 2: 역전파 알고리즘이란? -->
            <section>
                <h2>역전파 알고리즘이란?</h2>
                <div class="definition">
                    <p><strong>역전파(Backpropagation)</strong>는 신경망의 가중치와 바이어스를 학습시켜 손실 함수(오차)를 최소화하는 핵심 알고리즘입니다.</p>
                </div>
                <ul>
                    <li>신경망의 예측값과 실제값 사이의 오차를 계산</li>
                    <li>오차를 줄이는 방향으로 네트워크의 매개변수를 업데이트</li>
                    <li>출력층에서 입력층 방향으로 오차를 전파하며 학습</li>
                </ul>
            </section>

            <!-- 슬라이드 3: 핵심 원리 - 키워드 -->
            <section>
                <h2>역전파 알고리즘의 핵심 원리</h2>
                <div class="step-box">
                    <ul>
                        <li><span class="highlight">경사 하강법(Gradient Descent)</span>: 손실 함수를 최소화하기 위해 기울기(Gradient)를 따라 내려가는 방법</li>
                        <li><span class="highlight">연쇄 법칙(Chain Rule)</span>: 복합 함수의 미분을 각 구성 함수의 미분의 곱으로 계산하는 방법</li>
                        <li>순전파(Forward Propagation): 입력에서 출력까지 정방향으로 계산하는 과정</li>
                        <li>역전파(Backpropagation): 출력에서 입력으로 거꾸로 오차를 전파하는 과정</li>
                    </ul>
                </div>
            </section>

            <!-- 추가 슬라이드: 역전파의 개념 -->
            <section>
                <h2>역전파의 개념</h2>
                <div class="definition">
                    <p>역전파 알고리즘은 인공 신경망 학습의 핵심 메커니즘으로, <span class="key-point">네트워크의 오차를 줄이기 위해 가중치를 조정하는 방법</span>입니다.</p>
                </div>
                <div class="step-box">
                    <p>역전파 알고리즘의 특징:</p>
                    <ul>
                        <li>출력층에서 계산된 오차를 <strong>역방향으로 전파</strong>하며 각 층의 가중치를 업데이트</li>
                        <li>오차 함수의 그래디언트(기울기)를 계산하여 가중치 조정 방향 결정</li>
                        <li>연쇄 법칙을 효율적으로 적용하여 계산 비용 감소</li>
                    </ul>
                </div>
                <div class="note">
                    <p>역전파는 1986년 Geoffrey Hinton과 동료들에 의해 대중화되었으며, 딥러닝의 발전을 가능하게 한 핵심 알고리즘입니다.</p>
                </div>
            </section>

            <!-- 슬라이드 4: 핵심 원리 - 시각화 -->
            <section>
                <h2>역전파 시각화</h2>
                <div style="text-align: center;">
                    <a href="./backprop-visualization.html" target="_blank">시뮬레이션 보기</a>
                    
                </div>
                <div style="text-align: center;">
                    <img src="simulation-page-preview.png" alt="역전파 과정 시각화" style="width: 70%;">
                </div>
                
            </section>

            <!-- 신규 슬라이드 5-A: 신경망 구조 -->
            <section>
                <h2>신경망 구조</h2>
                <div class="step-box">
                    <p>기본적인 다층 퍼셉트론(MLP) 구조:</p>
                    <ul>
                        <li><strong>입력층(Input Layer)</strong>: 외부 데이터를 받아들이는 층</li>
                        <li><strong>은닉층(Hidden Layer)</strong>: 입력을 처리하는 중간 층 (여러 개 가능)</li>
                        <li><strong>출력층(Output Layer)</strong>: 최종 예측값을 생성하는 층</li>
                    </ul>
                </div>
                <div class="note">
                    <p>각 층은 노드(뉴런)로 구성되며, 노드 간 연결은 가중치(weight)를 가집니다.</p>
                    <p>역전파 알고리즘은 이 가중치들을 학습시키는 효율적인 방법입니다.</p>
                </div>
            </section>

            <!-- 신규 슬라이드 5-B: 학습 과정 개요 -->
            <section>
                <h2>역전파 학습 과정 개요</h2>
                <div class="two-column">
                    <div class="column">
                        <h3>학습 과정의 주요 단계:</h3>
                        <ol>
                            <li><span class="highlight">순전파(Forward Propagation)</span></li>
                            <li><span class="highlight">오차 계산(Error Calculation)</span></li>
                            <li><span class="highlight">역전파 과정(Backpropagation)</span></li>
                            <li><span class="highlight">가중치 업데이트(Weight Update)</span></li>
                        </ol>
                    </div>
                    <div class="column">
                        <div class="note">
                            <p>이 과정을 반복적으로 수행하며 네트워크의 성능을 점진적으로 향상시킵니다.</p>
                            <p>각 반복(에포크)마다 네트워크의 가중치는 오차를 줄이는 방향으로 조정됩니다.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- 슬라이드 5: 순전파 (1) -->
            <section>
                <h2>순전파(Forward Propagation) (1)</h2>
                <div class="step-box">
                    <h3>계산 과정</h3>
                    <ol>
                        <li>
                            <p>은닉층 뉴런의 가중합 ($z_1$):</p>
                            <div class="formula">
                                $$z_1 = x \cdot w_1 + b_1$$
                            </div>
                        </li>
                        <li>
                            <p>은닉층 뉴런의 활성화 출력 ($a_1$):</p>
                            <div class="formula">
                                $$a_1 = \sigma(z_1) = \frac{1}{1+e^{-z_1}}$$
                            </div>
                        </li>
                    </ol>
                </div>
                <div class="note">
                    <p>시그모이드 함수 $\sigma(z) = \frac{1}{1+e^{-z}}$는 입력을 0~1 사이의 값으로 변환합니다.</p>
                </div>
            </section>

            <!-- 슬라이드 6: 순전파 (2) -->
            <section>
                <h2>순전파 (Forward Propagation) - 2</h2>
                <div class="step-box">
                    <h3>계산 과정</h3>
                    <ol start="3">
                        <li>
                            <p>출력층 뉴런의 가중합 ($z_2$):</p>
                            <div class="formula">
                                $$z_2 = a_1 \cdot w_2 + b_2$$
                            </div>
                        </li>
                        <li>
                            <p>출력층 뉴런의 활성화 출력 ($o$):</p>
                            <div class="formula">
                                $$o = \sigma(z_2) = \frac{1}{1+e^{-z_2}}$$
                            </div>
                        </li>
                    </ol>
                </div>
                <div class="note">
                    <p>순전파는 입력에서 출력 방향으로 신호가 전달되는 과정입니다.</p>
                </div>
            </section>

            <!-- 슬라이드 7: 오차 계산 -->
            <section>
                <h2>오차 계산 (Error Calculation)</h2>
                <div class="formula">
                    $$E = \frac{1}{2}(y - o)^2$$
                </div>
                <p>여기서 $y$는 실제 타겟값, $o$는 신경망의 예측값입니다.</p>
                <div class="note">
                    <p>손실 함수의 선택은 문제 유형에 따라 달라질 수 있으며, 회귀 문제에서는 보통 평균 제곱 오차(MSE)를 사용합니다.</p>
                </div>
                <div class="step-box">
                    <p>신경망의 목표는 이 오차 값을 최소화하는 가중치와 바이어스를 찾는 것입니다.</p>
                </div>
            </section>

            <!-- 슬라이드 8: 역전파 - 개념 -->
            <section>
                <h2>역전파 (Backpropagation) - 개념</h2>
                <div class="note">
                    <p>역전파의 핵심은 <span class="key-point">출력층부터 입력층 방향으로</span> 오차를 전파하며 각 가중치와 바이어스에 대한 그래디언트를 계산하는 것입니다.</p>
                </div>
                <div class="step-box">
                    <h3>역전파의 핵심 단계</h3>
                    <ol>
                        <li>순전파로 예측값 계산</li>
                        <li>예측값과 실제값 사이의 오차 계산</li>
                        <li>출력층에서 입력층 방향으로 오차 기울기(그래디언트) 계산</li>
                        <li>그래디언트를 이용해 가중치와 바이어스 업데이트</li>
                    </ol>
                </div>
            </section>

            <!-- 슬라이드 9: 역전파 - 출력층 -->
            <section>
                <h2>역전파 - 출력층 계산</h2>
                <div class="chain-rule">
                    <div class="chain-rule-step">E</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">o</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">z₂</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">w₂</div>
                </div>
                <div class="formula">
                    $$\frac{\partial E}{\partial w_2} = \frac{\partial E}{\partial o} \cdot \frac{\partial o}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_2}$$
                </div>
                <div class="formula">
                    $$\frac{\partial E}{\partial w_2} = (o - y) \cdot o(1-o) \cdot a_1 = \delta_2 \cdot a_1$$
                </div>
                <div class="note">
                    <p>$\delta_2 = (o - y) \cdot o(1-o)$는 출력층의 오차 신호입니다.</p>
                </div>
            </section>

            <!-- 슬라이드 10: 역전파 - 은닉층 전파 -->
            <section>
                <h2>역전파 - 은닉층으로의 전파</h2>
                <div class="chain-rule">
                    <div class="chain-rule-step">E</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">o</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">z₂</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">a₁</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">z₁</div>
                    <div class="arrow">→</div>
                    <div class="chain-rule-step">w₁</div>
                </div>
                <div class="note">
                    <p>출력층의 오차가 은닉층으로 역전파되는 과정을 연쇄 법칙으로 계산합니다.</p>
                </div>
                <div class="formula">
                    $$\frac{\partial E}{\partial w_1} = \frac{\partial E}{\partial o} \cdot \frac{\partial o}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$$
                </div>
            </section>

            <!-- 연쇄 법칙과 편미분 슬라이드 - 여기로 이동 -->
            <section>
                <h2>연쇄 법칙과 편미분</h2>
                <div class="step-box">
                    <h3>편미분(Partial Derivative)</h3>
                    <p>여러 변수의 함수에서 하나의 변수에 대해서만 미분하는 것을 의미합니다.</p>
                    <div class="formula">
                        $$\frac{\partial f(x, y)}{\partial x} - \text{$y$를 상수로 취급하고 $x$에 대해서만 미분}$$
                    </div>
                </div>
                <div class="step-box">
                    <h3>연쇄 법칙(Chain Rule)</h3>
                    <p>합성 함수의 미분은 각 함수의 미분의 곱으로 표현됩니다.</p>
                    <div class="formula">
                        $$\frac{d}{dx}f(g(x)) = \frac{df}{dg} \cdot \frac{dg}{dx}$$
                    </div>
                    <p>역전파에서는 이 법칙을 통해 출력층의 오차가 각 층의 가중치에 미치는 영향을 효율적으로 계산합니다.</p>
                </div>
            </section>

            <!-- 시그모이드 함수와 미분 슬라이드 - 여기로 이동 -->
            <section>
                <h2>시그모이드 함수와 미분</h2>
                <div class="two-column">
                    <div class="column">
                        <div class="step-box">
                            <h3>시그모이드 함수(Sigmoid Function)</h3>
                            <div class="formula">
                                $$\sigma(z) = \frac{1}{1+e^{-z}}$$
                            </div>
                            <p>특징:</p>
                            <ul class="compact-text">
                                <li>입력값을 0~1 사이로 매핑</li>
                                <li>S자 형태의 부드러운 곡선</li>
                                <li>미분 가능한 비선형 함수</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column">
                        <div class="step-box">
                            <h3>시그모이드 함수의 미분</h3>
                            <div class="formula">
                                $$\frac{d\sigma(z)}{dz} = \sigma(z)(1-\sigma(z))$$
                            </div>
                            <p>증명:</p>
                            <div class="formula">
                                $$\begin{align*}
                                \frac{d\sigma(z)}{dz} &= \frac{d}{dz}\left(\frac{1}{1+e^{-z}}\right) \\
                                &= \frac{e^{-z}}{(1+e^{-z})^2} \\
                                &= \frac{1}{1+e^{-z}} \cdot \frac{e^{-z}}{1+e^{-z}} \\
                                &= \sigma(z) \cdot (1-\sigma(z))
                                \end{align*}$$
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- 슬라이드 11: 은닉층 오차 신호(δ₁) 계산 원리 - 순서 변경 -->
            <section>
                <h2>은닉층 오차 신호(δ₁) 계산 원리</h2>
                <div class="step-box">
                    <h3>연쇄 법칙(Chain Rule)을 통한 유도</h3>
                    <div class="formula">
                        $$\frac{\partial C}{\partial w_1} = \frac{\partial C}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$$
                    </div>
                </div>
                <div class="two-column">
                    <div class="column">
                        <p><strong>각 항의 의미:</strong></p>
                        <ul class="compact-text">
                            <li>$\frac{\partial z_1}{\partial w_1} = 입력값 x$: 가중합에 대한 가중치의 기여도</li>
                            <li>$\frac{\partial a_1}{\partial z_1} = a_1(1-a_1)$: 시그모이드 함수의 미분</li>
                            <li>$\frac{\partial C}{\partial a_1} = \frac{\partial C}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} = \delta_2 \cdot w_2$</li>
                        </ul>
                    </div>
                    <div class="column">
                        <div class="note">
                            <p>$\delta_1$은 출력층 오차가 은닉층에 미치는 영향을 나타내는 오차 신호입니다.</p>
                            <p>$\delta_1 = \delta_2 \cdot w_2 \cdot a_1(1-a_1)$</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- 슬라이드 12: 은닉층 오차 신호의 직관적 이해 -->
            <section>
                <h2>은닉층 오차 신호의 직관적 이해</h2>
                <div class="step-box">
                    <p>은닉층 오차 신호 $\delta_1$은 세 가지 요소의 곱으로 이해할 수 있습니다:</p>
                    <ul>
                        <li><strong>$\delta_2$</strong>: 출력층의 오차 신호 <span class="highlight">(얼마나 출력이 틀렸는지)</span></li>
                        <li><strong>$w_2$</strong>: 은닉층과 출력층 사이의 가중치 <span class="highlight">(얼마나 해당 은닉 노드가 출력에 영향을 주는지)</span></li>
                        <li><strong>$a_1(1-a_1)$</strong>: 활성화 함수의 미분값 <span class="highlight">(은닉 노드의 활성화 정도가 변화에 얼마나 민감한지)</span></li>
                    </ul>
                </div>
                <div class="note">
                    <p>이 과정은 "오차의 책임"이 네트워크를 통해 역방향으로 할당되는 방식을 보여줍니다.</p>
                    <p>각 노드가 최종 오차에 기여한 정도에 비례하여 가중치가 조정됩니다.</p>
                </div>
            </section>

            <!-- 슬라이드 13: 역전파 - 은닉층 계산 -->
            <section>
                <h2>역전파 - 은닉층 계산</h2>
                <div class="step-box">
                    <p>은닉층의 오차 신호 $\delta_1$을 이용하여 가중치와 바이어스를 업데이트하기 위한 그래디언트를 계산합니다:</p>
                    <div class="formula">
                        $$\frac{\partial E}{\partial w_1} = \delta_1 \cdot x$$
                    </div>
                    <div class="formula">
                        $$\frac{\partial E}{\partial b_1} = \delta_1$$
                    </div>
                </div>
            </section>

            <!-- 슬라이드 14: 은닉층 그래디언트 유도 과정 -->
            <section>
                <h2>은닉층 그래디언트 유도 과정</h2>
                <div class="step-box">
                    <h3>가중치($w_1$)에 대한 그래디언트 유도</h3>
                    <div class="formula">
                        $$\begin{align*}
                        \frac{\partial E}{\partial w_1} &= \frac{\partial E}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1} \\
                        &= \delta_2 \cdot w_2 \cdot a_1(1-a_1) \cdot x \\
                        &= \delta_1 \cdot x
                        \end{align*}$$
                    </div>
                    <p>여기서:</p>
                    <ul class="compact-text">
                        <li>$\frac{\partial E}{\partial a_1} = \delta_2 \cdot w_2$: 은닉층 출력에 대한 오차의 민감도</li>
                        <li>$\frac{\partial a_1}{\partial z_1} = a_1(1-a_1)$: 시그모이드 함수의 미분</li>
                        <li>$\frac{\partial z_1}{\partial w_1} = x$: 가중합에 대한 가중치의 영향</li>
                    </ul>
                </div>
            </section>

            <!-- 슬라이드 15: 은닉층 바이어스 그래디언트 유도 -->
            <section>
                <h2>은닉층 바이어스 그래디언트 유도</h2>
                <div class="step-box">
                    <div class="formula">
                        $$\begin{align*}
                        \frac{\partial E}{\partial b_1} &= \frac{\partial E}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial b_1} \\
                        &= \delta_2 \cdot w_2 \cdot a_1(1-a_1) \cdot 1 \\
                        &= \delta_1
                        \end{align*}$$
                    </div>
                    <p>여기서:</p>
                    <ul class="compact-text">
                        <li>$\frac{\partial E}{\partial a_1} = \delta_2 \cdot w_2$: 은닉층 출력에 대한 오차의 민감도</li>
                        <li>$\frac{\partial a_1}{\partial z_1} = a_1(1-a_1)$: 시그모이드 함수의 미분</li>
                        <li>$\frac{\partial z_1}{\partial b_1} = 1$: 가중합에 대한 바이어스의 영향 (바이어스는 항상 1을 곱해 더해짐)</li>
                    </ul>
                </div>
                <div class="note">
                    <p>가중치와 바이어스 그래디언트의 차이점은 $\frac{\partial z_1}{\partial w_1} = x$ 와 $\frac{\partial z_1}{\partial b_1} = 1$ 입니다.</p>
                    <p>이는 가중합 $z_1 = w_1 \cdot x + b_1$ 의 각 매개변수에 대한 미분에서 비롯됩니다.</p>
                </div>
            </section>

            <!-- 슬라이드 16: 가중치 업데이트 (1) -->
            <section>
                <h2>가중치 및 바이어스 업데이트 (1)</h2>
                <div class="step-box">
                    <h3>경사 하강법을 이용한 업데이트</h3>
                    <p>계산된 그래디언트를 사용하여 각 가중치와 바이어스를 업데이트합니다.</p>
                    <div class="formula">
                        $$w_1 \leftarrow w_1 - \alpha \frac{\partial E}{\partial w_1}$$
                    </div>
                    <div class="formula">
                        $$b_1 \leftarrow b_1 - \alpha \frac{\partial E}{\partial b_1}$$
                    </div>
                </div>
                <div class="note">
                    <p>여기서 $\alpha$는 학습률(learning rate)로, 업데이트 스텝의 크기를 조절합니다.</p>
                </div>
            </section>

            <!-- 슬라이드 17: 가중치 업데이트 (2) -->
            <section>
                <h2>가중치 및 바이어스 업데이트 (2)</h2>
                <div class="step-box">
                    <div class="formula">
                        $$w_2 \leftarrow w_2 - \alpha \frac{\partial E}{\partial w_2}$$
                    </div>
                    <div class="formula">
                        $$b_2 \leftarrow b_2 - \alpha \frac{\partial E}{\partial b_2}$$
                    </div>
                </div>
                <div class="note">
                    <p>학습률($\alpha$)이 너무 크면 발산할 수 있고, 너무 작으면 학습이 매우 느려집니다.</p>
                </div>
                <div class="step-box">
                    <p>이 과정을 여러 번 반복하면 오차가 점차 감소하며 네트워크가 학습됩니다.</p>
                </div>
            </section>

            <!-- 슬라이드 18: 수학적 원리 요약 -->
            <section>
                <h2>역전파 알고리즘의 수학적 원리 요약</h2>
                <table>
                    <tr>
                        <th>항목</th>
                        <th>수식</th>
                        <th>설명</th>
                    </tr>
                    <tr>
                        <td>출력층 오차 신호</td>
                        <td style="text-align: center">$\delta_2 = (o - y) \cdot o(1-o)$</td>
                        <td>출력층에서의 오차 기여도</td>
                    </tr>
                    <tr>
                        <td>은닉층 오차 신호</td>
                        <td style="text-align: center">$\delta_1 = \delta_2 \cdot w_2 \cdot a_1(1-a_1)$</td>
                        <td>역전파된 은닉층의 오차 기여도</td>
                    </tr>
                    <tr>
                        <td>출력층 가중치 그래디언트</td>
                        <td style="text-align: center">$\frac{\partial E}{\partial w_2} = \delta_2 \cdot a_1$</td>
                        <td>출력층 가중치에 대한 오차의 변화율</td>
                    </tr>
                    <tr>
                        <td>은닉층 가중치 그래디언트</td>
                        <td style="text-align: center">$\frac{\partial E}{\partial w_1} = \delta_1 \cdot x$</td>
                        <td>은닉층 가중치에 대한 오차의 변화율</td>
                    </tr>
                </table>
            </section>

            <!-- 슬라이드 19: 역전파의 장점 -->
            <section>
                <h2>역전파의 장점</h2>
                <div class="step-box">
                    <ul>
                        <li><strong>효율적인 그래디언트 계산</strong>: 수치 미분에 비해 훨씬 빠르고 정확하게 계산</li>
                        <li><strong>다층 신경망 학습 가능</strong>: 복잡한 다층 구조의 신경망 학습을 효율적으로 처리</li>
                        <li><strong>계산 복잡도 감소</strong>: 연쇄 법칙을 통해 중간 계산 결과를 재활용하여 계산량 감소</li>
                        <li><strong>행렬 연산 최적화</strong>: 병렬 처리가 가능한 행렬 연산으로 구현 가능</li>
                    </ul>
                </div>
                <div class="note">
                    <p>역전파는 딥러닝의 발전을 가능하게 한 핵심 알고리즘입니다.</p>
                </div>
            </section>

            <!-- 새로 추가: 구현 고려사항 슬라이드 -->
            <section>
                <h2>구현 고려사항</h2>
                <div class="two-column">
                    <div class="column">
                        <div class="step-box">
                            <h3>핵심 고려사항</h3>
                            <ul>
                                <li><span class="highlight">학습률(Learning Rate)</span>: 너무 크면 수렴하지 않고, 너무 작으면 학습이 느립니다.</li>
                                <li><span class="highlight">과적합 방지</span>: 드롭아웃, 정규화 등의 기법을 사용합니다.</li>
                                <li><span class="highlight">배치 크기</span>: 미니배치를 사용하여 계산 효율성과 일반화 성능을 높입니다.</li>
                                <li><span class="highlight">활성화 함수 선택</span>: ReLU, Tanh, Sigmoid 등 다양한 활성화 함수의 특성을 고려합니다.</li>
                            </ul>
                        </div>
                    </div>
                    <div class="column">
                        <div class="note">
                            <h3>최적화 기법</h3>
                            <ul class="compact-text">
                                <li>모멘텀(Momentum)</li>
                                <li>Adam, RMSprop 등의 적응적 학습률 최적화</li>
                                <li>배치 정규화(Batch Normalization)</li>
                                <li>그래디언트 클리핑(Gradient Clipping)</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </section>

            <!-- 슬라이드 20: 결론 (수정) -->
            <section>
                <h2>결론</h2>
                <div class="definition">
                    <p class="compact-text">역전파 알고리즘은 신경망 학습의 핵심으로, <span class="key-point">순전파로 오차를 계산한 뒤, 미분의 연쇄 법칙을 사용하여 오차가 각 층의 가중치와 바이어스에 미치는 영향(그래디언트)을 효율적으로 계산</span>하고, 이를 바탕으로 경사 하강법을 통해 네트워크를 학습시키는 방법입니다.</p>
                </div>
                <div class="step-box">
                    <p>역전파 알고리즘의 핵심 특징:</p>
                    <ul>
                        <li>연쇄 법칙을 활용하여 각 가중치가 전체 오차에 기여한 정도를 계산</li>
                        <li>오차 신호가 출력층에서 입력층으로 역방향으로 전파됨</li>
                        <li>중간 계산 결과를 재활용하여 계산 효율성 향상</li>
                        <li>복잡한 패턴을 학습할 수 있는 강력한 메커니즘 제공</li>
                    </ul>
                </div>
                <div class="note">
                    <p>이 알고리즘은 심층 신경망 학습의 기초가 되며, 네트워크를 점진적으로 개선하는 체계적인 방법을 제공합니다.</p>
                </div>
            </section>

            <section data-background="linear-gradient(to right, #3498db, #2c3e50)">
                <h1 style="color: white;">감사합니다</h1>
                <h3 style="color: #f0f0f0;">질문이 있으신가요?</h3>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/notes/notes.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/markdown/markdown.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/math/math.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            slideNumber: true,
            transition: 'slide',
            transitionSpeed: 'fast',
            center: false,
            width: 1280,
            height: 720,
            margin: 0.05,
            minScale: 0.2,
            maxScale: 2.0,
            plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
        });
    </script>
</body>
</html> 