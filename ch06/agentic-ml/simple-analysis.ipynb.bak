{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.types.responses import (ResponseTextDeltaEvent, ResponseFunctionCallArgumentsDeltaEvent)\n",
    "from agents import Agent, Runner\n",
    "from agents import function_tool\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def kaggle_list_datasets(keyword: str) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    검색 쿼리에 맞는 Kaggle 데이터셋 목록을 조회합니다\n",
    "    \"\"\"\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    try:\n",
    "        # 데이터셋 검색\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        datasets = api.dataset_list(search=keyword, sort_by=\"hottest\")\n",
    "        \n",
    "        result = []\n",
    "        count = 0\n",
    "        for dataset in datasets:\n",
    "            if count >= 10:\n",
    "                break\n",
    "            dataset_info = {\n",
    "                \"ref\": dataset.ref,  # owner/dataset-name 형식\n",
    "                \"title\": dataset.title,\n",
    "            }\n",
    "            \n",
    "            # 안전하게 속성 추가\n",
    "            if hasattr(dataset, 'size'):\n",
    "                dataset_info[\"size\"] = dataset.size\n",
    "            if hasattr(dataset, 'lastUpdated'):\n",
    "                dataset_info[\"last_updated\"] = str(dataset.lastUpdated)\n",
    "            if hasattr(dataset, 'downloadCount'):\n",
    "                dataset_info[\"download_count\"] = dataset.downloadCount\n",
    "            if hasattr(dataset, 'voteCount'):\n",
    "                dataset_info[\"vote_count\"] = dataset.voteCount\n",
    "            if hasattr(dataset, 'tags'):\n",
    "                dataset_info[\"tags\"] = dataset.tags\n",
    "            if hasattr(dataset, 'usabilityRating'):\n",
    "                dataset_info[\"usability_rating\"] = dataset.usabilityRating\n",
    "            \n",
    "            result.append(dataset_info)\n",
    "            count += 1\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"count\": len(result),\n",
    "            \"datasets\": result\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"message\": f\"데이터셋 목록 조회 실패: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def kaggle_check_and_download_dataset(dataset_ref: str, path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    데이터셋이 이미 다운로드되어 있는지 확인하고, 없으면 다운로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        dataset_ref: 다운로드할 데이터셋의 참조 (owner/dataset-name 형식)\n",
    "        path: 데이터셋을 저장할 경로 (기본값: None, Kaggle API 기본 경로 사용)\n",
    "    \n",
    "    Returns:\n",
    "        데이터셋 상태 및 경로 정보\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    try:\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        # 데이터셋 정보 추출\n",
    "        owner, dataset_name = dataset_ref.split('/')\n",
    "        \n",
    "        # 기본 Kaggle 다운로드 경로 또는 사용자 지정 경로\n",
    "        if path is None:\n",
    "            from pathlib import Path\n",
    "            kaggle_dir = os.path.join(str(Path.home()), '.kaggle', 'datasets')\n",
    "            dataset_dir = os.path.join(kaggle_dir, owner, dataset_name)\n",
    "        else:\n",
    "            dataset_dir = os.path.join(path, owner, dataset_name)\n",
    "        \n",
    "        # 데이터셋이 이미 존재하는지 확인\n",
    "        if os.path.exists(dataset_dir) and len(os.listdir(dataset_dir)) > 0:\n",
    "            # 데이터셋이 이미 존재함\n",
    "            files = os.listdir(dataset_dir)\n",
    "            data_storage[dataset_ref] = dataset_dir\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"already_downloaded\": True,\n",
    "                \"message\": f\"데이터셋 '{dataset_ref}'는 이미 다운로드되어 있습니다.\",\n",
    "                \"path\": dataset_dir,\n",
    "                \"files\": files\n",
    "            }\n",
    "        else:\n",
    "            # 데이터셋 다운로드\n",
    "            api.dataset_download_files(dataset_ref, path=path, unzip=True)\n",
    "            \n",
    "            # 다운로드 후 경로 확인 및 저장\n",
    "            if path is None:\n",
    "                # 기본 경로에 다운로드된 경우\n",
    "                dataset_dir = os.path.join(kaggle_dir, owner, dataset_name)\n",
    "            \n",
    "            if os.path.exists(dataset_dir):\n",
    "                files = os.listdir(dataset_dir)\n",
    "                data_storage[dataset_ref] = dataset_dir\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"already_downloaded\": False,\n",
    "                    \"message\": f\"데이터셋 '{dataset_ref}'를 성공적으로 다운로드했습니다.\",\n",
    "                    \"path\": dataset_dir,\n",
    "                    \"files\": files\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"message\": f\"데이터셋 '{dataset_ref}'를 다운로드했지만 경로를 찾을 수 없습니다.\"\n",
    "                }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"message\": f\"데이터셋 다운로드 중 오류 발생: {str(e)}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def kaggle_download_dataset(dataset_ref: str) -> str:\n",
    "    \"\"\"\n",
    "    Kaggle 데이터셋을 다운로드합니다\n",
    "    \"\"\"\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    try:\n",
    "        # Kaggle API로 데이터셋 다운로드\n",
    "        api.dataset_download_files(dataset_ref, path=f\"datasets/{dataset_ref}/\", unzip=True)\n",
    "        import os\n",
    "        import pandas as pd\n",
    "        \n",
    "        # 다운로드 성공 시 처리\n",
    "        dataset_path = f\"datasets/{dataset_ref}/\"\n",
    "        \n",
    "        # 디렉토리 내 파일 목록 가져오기\n",
    "        files = os.listdir(dataset_path)\n",
    "        \n",
    "        # CSV 파일 찾기\n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        \n",
    "        if csv_files:\n",
    "            # 첫 번째 CSV 파일 로드\n",
    "            first_csv = os.path.join(dataset_path, csv_files[0])\n",
    "            df = pd.read_csv(first_csv)\n",
    "            \n",
    "            # 데이터 저장소에 저장\n",
    "            data_storage[dataset_ref] = df\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"데이터셋 다운로드 및 로드 성공 : (저장소 key : {dataset_ref})\",\n",
    "                \"file_loaded\": csv_files[0],\n",
    "                \"rows\": len(df),\n",
    "                \"columns\": len(df.columns)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"데이터셋 다운로드 성공했으나 CSV 파일이 없습니다: {dataset_ref}\",\n",
    "                \"files_available\": files\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"message\": f\"데이터셋 다운로드 실패: {str(e)}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def load_csv_from_url(url: str, key: str) -> str:\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    URL에서 CSV 파일을 읽어 Pandas DataFrame으로 변환하고 메모리에 저장\n",
    "    \n",
    "    Args:\n",
    "        url (str): CSV 파일 주소\n",
    "        key (str): 저장할 키\n",
    "        \n",
    "    Returns:\n",
    "        str: 작업 결과 메시지\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        data_storage[key] = df\n",
    "        return f\"CSV 파일이 성공적으로 로드됨 (행: {len(df)}, 열: {len(df.columns)})\"\n",
    "    except Exception as e:\n",
    "        return f\"오류 발생: {str(e)}\"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def show_df_stats(key: str) -> str:\n",
    "    \"\"\"저장된 데이터프레임의 기본 통계 정보 출력\n",
    "    저장된 데이터 형태  (key) : Pandas DataFrame\n",
    "    \"\"\"\n",
    "    if key not in data_storage:\n",
    "        return \"해당 URL의 CSV 파일을 찾을 수 없음\"\n",
    "    \n",
    "    df = data_storage[key]\n",
    "    return f\"\"\"\n",
    "    행 수: {len(df)}\n",
    "    열 수: {len(df.columns)}\n",
    "    결측치 수: {df.isnull().sum().sum()}\n",
    "    컬럼 목록: {df.columns.tolist()}\n",
    "    컬럼 타입: {df.dtypes.to_dict()}\n",
    "    기초 통계: {df.describe().to_dict()}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def random_sample(key: str, n: int) -> str:\n",
    "    \"\"\"저장된 데이터프레임에서 랜덤 샘플 추출\n",
    "    저장된 데이터 형태  (key) : Pandas DataFrame\n",
    "    \"\"\"\n",
    "    if key not in data_storage:\n",
    "        return \"해당 URL의 CSV 파일을 찾을 수 없음\"\n",
    "    new_key = str(uuid.uuid4())\n",
    "    df = data_storage[key]\n",
    "    random_sample_df = df.sample(n)\n",
    "    data_storage[new_key] = random_sample_df\n",
    "    return f\"랜덤 샘플 추출 완료: (저장소 key : {new_key})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def prep_df(key: str, target_column: str) -> str:\n",
    "    \"\"\"저장된 데이터프레임을 머신러닝 모델에 적합한 형태로 전처리\n",
    "    저장된 데이터 형태  (key) : Pandas DataFrame\n",
    "    새로 저장할 데이터 형태 (new_key) : (X, y) 형태의 튜플\n",
    "    \"\"\"\n",
    "    new_key = str(uuid.uuid4())\n",
    "    df = data_storage[key]\n",
    "    if target_column not in df.columns:\n",
    "        return f\"해당 컬럼 {target_column}이 존재하지 않습니다.\"\n",
    "    df = df[df[target_column].notna()]\n",
    "    \n",
    "    # 실수형 데이터만 남기기\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df = df[numeric_cols]\n",
    "    \n",
    "    if target_column not in df.columns:\n",
    "        return f\"타겟 컬럼 {target_column}이 실수형 데이터가 아니어서 제거되었습니다.\"\n",
    "    \n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    data_storage[new_key] = (X, y)\n",
    "    return f\"데이터프레임 전처리 완료 (실수형 데이터만 포함): (저장소 key : {new_key})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def visualize_tsne(key: str) -> str:\n",
    "    \"\"\"\n",
    "    저장된 데이터프레임을 t-SNE로 시각화\n",
    "    저장된 데이터 형태 (key) : (X, y) 형태의 튜플\n",
    "    새로 저장할 데이터 형태 (new_key) : Plotly 객체\n",
    "    \"\"\"\n",
    "    from utils import TSNEVisualizer\n",
    "    import plotly.io as pio\n",
    "    from IPython.display import HTML\n",
    "    new_key = str(uuid.uuid4())\n",
    "    \n",
    "    # 렌더러 설정\n",
    "    pio.renderers.default = \"notebook\"\n",
    "    \n",
    "    (X, y) = data_storage[key]\n",
    "    fig = TSNEVisualizer.visualize(X, y)\n",
    "    \n",
    "    # to_html 메서드로 HTML 문자열 생성하여 표시\n",
    "    html_str = fig.to_html(include_plotlyjs='cdn')\n",
    "    display(HTML(html_str))\n",
    "    \n",
    "    data_storage[new_key] = fig\n",
    "    \n",
    "    return f\"t-SNE 시각화 완료: (저장소 key : {new_key})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def kaggle_check_and_download_dataset(dataset_ref: str, path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    데이터셋이 이미 다운로드되어 있는지 확인하고, 없으면 다운로드합니다.\n",
    "    \n",
    "    Args:\n",
    "        dataset_ref: 다운로드할 데이터셋의 참조 (owner/dataset-name 형식)\n",
    "        path: 데이터셋을 저장할 경로 (기본값: None, Kaggle API 기본 경로 사용)\n",
    "    \n",
    "    Returns:\n",
    "        데이터셋 상태 및 경로 정보\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    \n",
    "    try:\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        # 데이터셋 정보 추출\n",
    "        owner, dataset_name = dataset_ref.split('/')\n",
    "        \n",
    "        # 기본 Kaggle 다운로드 경로 또는 사용자 지정 경로\n",
    "        if path is None:\n",
    "            from pathlib import Path\n",
    "            kaggle_dir = os.path.join(str(Path.home()), '.kaggle', 'datasets')\n",
    "            dataset_dir = os.path.join(kaggle_dir, owner, dataset_name)\n",
    "        else:\n",
    "            dataset_dir = os.path.join(path, owner, dataset_name)\n",
    "        \n",
    "        # 데이터셋이 이미 존재하는지 확인\n",
    "        if os.path.exists(dataset_dir) and len(os.listdir(dataset_dir)) > 0:\n",
    "            # 데이터셋이 이미 존재함\n",
    "            files = os.listdir(dataset_dir)\n",
    "            data_storage[dataset_ref] = dataset_dir\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"already_downloaded\": True,\n",
    "                \"message\": f\"데이터셋 '{dataset_ref}'는 이미 다운로드되어 있습니다.\",\n",
    "                \"path\": dataset_dir,\n",
    "                \"files\": files\n",
    "            }\n",
    "        else:\n",
    "            # 데이터셋 다운로드\n",
    "            api.dataset_download_files(dataset_ref, path=path, unzip=True)\n",
    "            \n",
    "            # 다운로드 후 경로 확인 및 저장\n",
    "            if path is None:\n",
    "                # 기본 경로에 다운로드된 경우\n",
    "                dataset_dir = os.path.join(kaggle_dir, owner, dataset_name)\n",
    "            \n",
    "            if os.path.exists(dataset_dir):\n",
    "                files = os.listdir(dataset_dir)\n",
    "                data_storage[dataset_ref] = dataset_dir\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"already_downloaded\": False,\n",
    "                    \"message\": f\"데이터셋 '{dataset_ref}'를 성공적으로 다운로드했습니다.\",\n",
    "                    \"path\": dataset_dir,\n",
    "                    \"files\": files\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"message\": f\"데이터셋 '{dataset_ref}'를 다운로드했지만 경로를 찾을 수 없습니다.\"\n",
    "                }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"message\": f\"데이터셋 다운로드 중 오류 발생: {str(e)}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def under_resample(key: str) -> str:\n",
    "    \"\"\"\n",
    "    저장된 데이터프레임을 언더리샘플링\n",
    "    저장된 데이터 형태  (key) : (X, y) 형태의 튜플\n",
    "    새로 저장할 데이터 형태 (new_key) : (X, y) 형태의 튜플\n",
    "    \"\"\"\n",
    "    from utils import ImbalancedDataAnalyzer\n",
    "    new_key = str(uuid.uuid4())\n",
    "    (X, y) = data_storage[key]\n",
    "    analyzer = ImbalancedDataAnalyzer(X, y)\n",
    "    X_resampled, y_resampled = analyzer.random_undersample()\n",
    "    data_storage[new_key] = (X_resampled, y_resampled)\n",
    "    return f\"SMOTE 리샘플링 완료: (저장소 key : {new_key})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def isolation_forest(key: str, contamination: float) -> str:\n",
    "    \"\"\"\n",
    "    저장된 데이터프레임을 Isolation Forest로 분석\n",
    "    저장된 데이터 형태  (key) : (X, y) 형태의 튜플\n",
    "    새로 저장할 데이터 형태 (new_key) : (y, score) 형태의 튜플\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import plotly.express as px\n",
    "    new_key = str(uuid.uuid4())\n",
    "    \n",
    "    (X, y) = data_storage[key]\n",
    "    model = IsolationForest(contamination=contamination, max_samples=256)\n",
    "    model.fit(X)\n",
    "    score = model.score_samples(X)\n",
    "    score = -1 * score  # 이부분임.\n",
    "    fig = px.histogram(x=score, nbins=100, labels={'x':'Score'}, title=\"평가\")\n",
    "    fig.update_layout(width=600, height=400)\n",
    "    fig.show()\n",
    "    data_storage[new_key] = (y, score)\n",
    "    return f\"Isolation Forest 분석 완료: (Score 저장소 key : {new_key})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def prec_rec_f1_curve(key: str):\n",
    "    \"\"\"\n",
    "    저장된 데이터프레임을 Precision, Recall, F1 Score 곡선으로 시각화\n",
    "    저장된 데이터 형태  (key) : (y, score) 형태의 튜플\n",
    "    새로 저장할 데이터 형태 (new_key) : (precision, recall, f1, thresholds) 형태의 튜플\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import plotly.graph_objects as go\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    new_key = str(uuid.uuid4())\n",
    "    y, score = data_storage[key]\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y, score, pos_label=1)\n",
    "    f1 = 2 / (1/precision + 1/recall)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=np.delete(precision, -1), mode='lines', name='precision'))\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=np.delete(recall, -1), mode='lines', name='recall'))\n",
    "    fig.add_trace(go.Scatter(x=thresholds, y=np.delete(f1, -1), mode='lines', name='f1'))\n",
    "    fig.update_layout(title='Anomaly Score에 따른 Precision, Recall, F1 Score',\n",
    "                      xaxis_title='Anomaly Score',\n",
    "                      yaxis_title='Score',\n",
    "                      legend_title='Score Type')\n",
    "    fig.show()\n",
    "    \n",
    "    data_storage[new_key] = (precision, recall, f1, thresholds)\n",
    "\n",
    "    return precision, recall, f1, thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_tool\n",
    "def show_tool_list() -> str:\n",
    "    \"\"\"\n",
    "    현재 사용 가능한 도구 목록을 출력\n",
    "    \"\"\"\n",
    "    tool_names = [\n",
    "        \"show_tool_list\"\n",
    "        \"kaggle_list_datasets\",\n",
    "        \"kaggle_download_dataset\",\n",
    "        \"show_df_stats\",\n",
    "        \"random_sample\",\n",
    "        \"prep_df\",\n",
    "        \"visualize_tsne\",\n",
    "        \"under_resample\",\n",
    "        \"isolation_forest\",\n",
    "        \"prec_rec_f1_curve\",\n",
    "        \n",
    "    ]\n",
    "    return f\"현재 사용 가능한 도구 목록: {', '.join(tool_names)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\n",
    "        \"너는 데이터 분석 전문가야\"\n",
    "        \"가능한 한 항상 제공된 도구를 사용하는 것을 기억해야되.\"\n",
    "        \"너가 사용가능한 도구 목록을 먼저 출력해놓고 시작해. show_tool_list 도구를 사용하면 됨.\"\n",
    "        \"자신의 지식에 너무 의존하지 말고 대신 질문에 답하는 데 도움이 되는 도구를 사용하세요.\"\n",
    "        \"python 코드를 직접구현하는 방식을 사용하지 말고, 도구에 의존하세요.\"\n",
    "        \"t-SNE 시각화나 다른 시각화를 사용할 때 샘플 수가 적어 'perplexity must be less than n_samples' 등의 오류가 발생하면, \"\n",
    "        \"해당 시각화 단계를 건너뛰고 사용자에게 샘플 수가 부족하여 시각화가 불가능하다고 알려주세요.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[\n",
    "        show_tool_list,\n",
    "        kaggle_list_datasets,\n",
    "        kaggle_download_dataset,\n",
    "        show_df_stats,\n",
    "        random_sample,\n",
    "        prep_df,\n",
    "        visualize_tsne,\n",
    "        under_resample,\n",
    "        isolation_forest,\n",
    "        prec_rec_f1_curve,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 현재 에이전트: Assistant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error streaming response: Project `proj_9jPb3KpbTdfDPtpFQRloSjOo` does not have access to model `gpt-4o-mini-2024-07-18`\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Project `proj_9jPb3KpbTdfDPtpFQRloSjOo` does not have access to model `gpt-4o-mini-2024-07-18`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# we do need to reinitialize our runner before re-executing\u001b[39;00m\n\u001b[32m      2\u001b[39m response = Runner.run_streamed(\n\u001b[32m      3\u001b[39m     starting_agent=agent,\n\u001b[32m      4\u001b[39m     \u001b[38;5;28minput\u001b[39m=\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m10. Isolation Forest 분석한 데이터를 Precision, Recall, F1 Score 곡선으로 시각화해줘.\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m response.stream_events():\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event.type == \u001b[33m\"\u001b[39m\u001b[33mraw_response_event\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n\u001b[32m     20\u001b[39m             \u001b[38;5;66;03m# 도구 호출을 위한 스트리밍 매개변수\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dante-code/projects/kmu-practical-ds-2025/.conda/lib/python3.12/site-packages/agents/result.py:205\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dante-code/projects/kmu-practical-ds-2025/.conda/lib/python3.12/site-packages/agents/run.py:560\u001b[39m, in \u001b[36mRunner._run_streamed_impl\u001b[39m\u001b[34m(cls, starting_input, streamed_result, starting_agent, max_turns, hooks, context_wrapper, run_config, previous_response_id)\u001b[39m\n\u001b[32m    549\u001b[39m     streamed_result._input_guardrails_task = asyncio.create_task(\n\u001b[32m    550\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails_with_queue(\n\u001b[32m    551\u001b[39m             starting_agent,\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m         )\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn_streamed(\n\u001b[32m    561\u001b[39m         streamed_result,\n\u001b[32m    562\u001b[39m         current_agent,\n\u001b[32m    563\u001b[39m         hooks,\n\u001b[32m    564\u001b[39m         context_wrapper,\n\u001b[32m    565\u001b[39m         run_config,\n\u001b[32m    566\u001b[39m         should_run_agent_start_hooks,\n\u001b[32m    567\u001b[39m         tool_use_tracker,\n\u001b[32m    568\u001b[39m         all_tools,\n\u001b[32m    569\u001b[39m         previous_response_id,\n\u001b[32m    570\u001b[39m     )\n\u001b[32m    571\u001b[39m     should_run_agent_start_hooks = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    573\u001b[39m     streamed_result.raw_responses = streamed_result.raw_responses + [\n\u001b[32m    574\u001b[39m         turn_result.model_response\n\u001b[32m    575\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dante-code/projects/kmu-practical-ds-2025/.conda/lib/python3.12/site-packages/agents/run.py:671\u001b[39m, in \u001b[36mRunner._run_single_turn_streamed\u001b[39m\u001b[34m(cls, streamed_result, agent, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, all_tools, previous_response_id)\u001b[39m\n\u001b[32m    668\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m streamed_result.new_items])\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# 1. Stream the output events\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m model.stream_response(\n\u001b[32m    672\u001b[39m     system_prompt,\n\u001b[32m    673\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    674\u001b[39m     model_settings,\n\u001b[32m    675\u001b[39m     all_tools,\n\u001b[32m    676\u001b[39m     output_schema,\n\u001b[32m    677\u001b[39m     handoffs,\n\u001b[32m    678\u001b[39m     get_model_tracing_impl(\n\u001b[32m    679\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    680\u001b[39m     ),\n\u001b[32m    681\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    682\u001b[39m ):\n\u001b[32m    683\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ResponseCompletedEvent):\n\u001b[32m    684\u001b[39m         usage = (\n\u001b[32m    685\u001b[39m             Usage(\n\u001b[32m    686\u001b[39m                 requests=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Usage()\n\u001b[32m    693\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dante-code/projects/kmu-practical-ds-2025/.conda/lib/python3.12/site-packages/agents/models/openai_responses.py:157\u001b[39m, in \u001b[36mOpenAIResponsesModel.stream_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m    144\u001b[39m stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m    145\u001b[39m     system_instructions,\n\u001b[32m    146\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    152\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m final_response: Response | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, ResponseCompletedEvent):\n\u001b[32m    159\u001b[39m         final_response = chunk.response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dante-code/projects/kmu-practical-ds-2025/.conda/lib/python3.12/site-packages/openai/_streaming.py:147\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/dante-code/projects/kmu-practical-ds-2025/.conda/lib/python3.12/site-packages/openai/_streaming.py:193\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    191\u001b[39m                 message = \u001b[33m\"\u001b[39m\u001b[33mAn error occurred during streaming\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m    194\u001b[39m                 message=message,\n\u001b[32m    195\u001b[39m                 request=\u001b[38;5;28mself\u001b[39m.response.request,\n\u001b[32m    196\u001b[39m                 body=data[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    197\u001b[39m             )\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m process_data(data={\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: data, \u001b[33m\"\u001b[39m\u001b[33mevent\u001b[39m\u001b[33m\"\u001b[39m: sse.event}, cast_to=cast_to, response=response)\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Ensure the entire stream is consumed\u001b[39;00m\n",
      "\u001b[31mAPIError\u001b[39m: Project `proj_9jPb3KpbTdfDPtpFQRloSjOo` does not have access to model `gpt-4o-mini-2024-07-18`"
     ]
    }
   ],
   "source": [
    "# we do need to reinitialize our runner before re-executing\n",
    "response = Runner.run_streamed(\n",
    "    starting_agent=agent,\n",
    "    input=\n",
    "        \"1. 사기 이상탐지 관련 데이터셋을 캐글에서 조회한후,\"\n",
    "        \"2. 첫번째 데이터셋을 다운로드해줘.\"\n",
    "        \"3. 다운로드 받은 데이터셋에서 3000개 레코드만 랜덤 샘플링해줘.\"\n",
    "        \"4. 3000개 레코드의 기초 통계량을 출력해줘.\"\n",
    "        \"5. 앞에서 처리한 랜덤 샘플링한 3000개 레코드를 머신러닝 모델에 적합한 형태로 전처리해줘.\"\n",
    "        \"6. 전처리한 데이터를 TSNE 시각화해줘.\"\n",
    "        \"7. 전처리한 데이터를 SMOTE 리샘플링을 해줘.\"\n",
    "        \"8. SMOTE 리샘플링한 데이터를 TSNE 시각화해줘.\"\n",
    "        \"9. SMOTE 리샘플링한 데이터를 Isolation Forest 분석해줘.\"\n",
    "        \"10. Isolation Forest 분석한 데이터를 Precision, Recall, F1 Score 곡선으로 시각화해줘.\" \n",
    ")\n",
    "\n",
    "async for event in response.stream_events():\n",
    "    if event.type == \"raw_response_event\":\n",
    "        if isinstance(event.data, ResponseFunctionCallArgumentsDeltaEvent):\n",
    "            # 도구 호출을 위한 스트리밍 매개변수\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "        elif isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            # 스트리밍된 최종 응답 토큰\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "    elif event.type == \"agent_updated_stream_event\":\n",
    "        # 현재 사용 중인 에이전트 정보\n",
    "        print(f\"> 현재 에이전트: {event.new_agent.name}\")\n",
    "    elif event.type == \"run_item_stream_event\":\n",
    "        # 사용자나 다운스트림 프로세스로 스트리밍할 정보를 포함하는 이벤트\n",
    "        if event.name == \"tool_called\":\n",
    "            # 모든 도구 토큰이 스트리밍된 후 전체 도구 호출 정보\n",
    "            print()\n",
    "            print(f\"> 도구 호출됨, 이름: {event.item.raw_item.name}\")\n",
    "            print(f\"> 도구 호출됨, 인자: {event.item.raw_item.arguments}\")\n",
    "        elif event.name == \"tool_output\":\n",
    "            # 도구 실행 결과\n",
    "            print(f\"> 도구 출력: {event.item.raw_item['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_storage.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_storage['e979a416-372c-4766-8a37-8189f4ba7d49']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
